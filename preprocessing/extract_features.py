import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import timm
from huggingface_hub import login

# ==========================================
# [ì„¤ì •] ëª¨ë¸ë³„ ìƒì„¸ ìŠ¤í™ (HuggingFace ID ë“±)
# ==========================================
MODEL_CONFIGS = {
    "uni": {
        "hf_id": "MahmoodLab/uni",
        "arch": "vit_large_patch16_224",
        "img_size": 224,
        "mean": (0.5, 0.5, 0.5), # UNIëŠ” ë³´í†µ 0.5 normalize ê¶Œì¥ì¸ ê²½ìš°ê°€ ë§ìŒ (í™•ì¸ í•„ìš”ì‹œ ì¡°ì •)
        "std": (0.5, 0.5, 0.5),
    },
    "virchow2": {
        "hf_id": "paige-ai/Virchow2",
        "arch": "hf_hub:paige-ai/Virchow2", # timmì´ HF Hub ì§€ì›
        "img_size": 224, # VirchowëŠ” 224 or 512 (ë³´í†µ 224 íŒ¨ì¹˜ ì‚¬ìš©ì‹œ)
        "mean": (0.485, 0.456, 0.406), # ImageNet Stat
        "std": (0.229, 0.224, 0.225),
    },
    "conch": {
        "hf_id": "MahmoodLab/CONCH", 
        "script_path": None, # CONCHëŠ” ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”í•  ìˆ˜ ìˆìŒ (ì•„ë˜ ì„¤ëª… ì°¸ì¡°)
        "img_size": 224,
        "mean": (0.48145466, 0.4578275, 0.40821073), # CLIP standard
        "std": (0.26862954, 0.26130258, 0.27577711),
    }
}

class PatchDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        
        # ì¬ê·€ì ìœ¼ë¡œ ì´ë¯¸ì§€ íŒŒì¼ ê²€ìƒ‰
        print(f"ğŸ” Scanning files in {root_dir}...")
        for root, _, files in os.walk(root_dir):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')):
                    self.image_paths.append(os.path.join(root, file))
        print(f"   ğŸ‘‰ Found {len(self.image_paths)} images.")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, img_path
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            return torch.zeros((3, 224, 224)), img_path # Dummy for error handling

def get_model_and_transform(model_name):
    print(f"ğŸš€ Loading Model: {model_name.upper()}...")
    cfg = MODEL_CONFIGS[model_name]
    
    # 1. Transform ì„¤ì •
    transform = transforms.Compose([
        transforms.Resize((cfg['img_size'], cfg['img_size'])),
        transforms.ToTensor(),
        transforms.Normalize(mean=cfg['mean'], std=cfg['std']),
    ])
    
    # 2. Model ë¡œë“œ
    model = None
    
    if model_name == "uni":
        # UNI: timmì„ í†µí•´ ë¡œë“œ (hf_hub ì§€ì›)
        # ë§Œì•½ ë¡œì»¬ ê°€ì¤‘ì¹˜ê°€ ìˆë‹¤ë©´ pretrained_cfg ë“±ì„ ìˆ˜ì •í•´ì•¼ í•¨
        model = timm.create_model("hf_hub:MahmoodLab/uni", pretrained=True, init_values=1e-5, dynamic_img_size=True)
        
    elif model_name == "virchow2":
        # Virchow2: timm + HF Hub
        # ì£¼ì˜: timm ìµœì‹  ë²„ì „ í•„ìš”
        model = timm.create_model("hf_hub:paige-ai/Virchow2", pretrained=True, mlp_layer=timm.layers.SwiGLUPacked, act_layer=torch.nn.SiLU)
        
    elif model_name == "conch":
        # CONCHëŠ” timmìœ¼ë¡œ ë°”ë¡œ ì•ˆ ë  ìˆ˜ ìˆìŒ. open_clipì´ë‚˜ ë³„ë„ ë¡œë” í•„ìš”.
        # ì—¬ê¸°ì„œëŠ” timm í˜¸í™˜ì´ ëœë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, í˜¹ì€ ì‚¬ìš©ì í™˜ê²½ì— conch ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆë‹¤ê³  ê°€ì •.
        try:
            from conch.open_clip_custom import create_model_from_pretrained
            model, _ = create_model_from_pretrained('conch_ViT-B-16', "hf_hub:MahmoodLab/CONCH")
        except ImportError:
            print("ğŸš¨ [Error] CONCH requires 'conch' library installed or specific loader.")
            print("   -> Fallback: Attempting generic ViT loading (Might fail for CONCH specifics)")
            # CONCHê°€ ViT-B/16 ê¸°ë°˜ì´ë¯€ë¡œ êµ¬ì¡°ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìœ¼ë‚˜, ê°€ì¤‘ì¹˜ ë§¤í•‘ì´ ë‹¤ë¦„.
            # *ì‹¤ì œ ì‚¬ìš©ì‹œì—ëŠ” MahmoodLabì˜ ê³µì‹ conch repo ì½”ë“œë¥¼ ì°¸ì¡°í•´ì•¼ í•¨*
            raise NotImplementedError("CONCH loading requires custom library installation.")

    # ê³µí†µ: Evaluation ëª¨ë“œ & Head ì œê±° (Feature Extraction ìš©)
    if hasattr(model, 'reset_classifier'):
        model.reset_classifier(0) # Remove classification head
    
    model.eval()
    model.cuda()
    return model, transform

def main():
    parser = argparse.ArgumentParser(description="Extract Features using Foundation Models")
    parser.add_argument("--input_dir", type=str, required=True, help="Input directory containing patch images")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory to save .pt files")
    parser.add_argument("--model", type=str, default="uni", choices=["uni", "virchow2", "conch"], help="Model to use")
    parser.add_argument("--batch_size", type=int, default=128, help="Batch size")
    args = parser.parse_args()

    # ì¶œë ¥ í´ë” ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ëª¨ë¸ & ë°ì´í„°ì…‹ ì¤€ë¹„
    model, transform = get_model_and_transform(args.model)
    dataset = PatchDataset(args.input_dir, transform=transform)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, num_workers=8, shuffle=False)
    
    print("ğŸ¬ Start Extraction...")
    with torch.no_grad():
        for images, paths in tqdm(dataloader):
            images = images.cuda()
            
            # ëª¨ë¸ë³„ Forward ë°©ì‹ ì°¨ì´ ì²˜ë¦¬
            if args.model == "conch":
                # CONCHëŠ” visual encoderë§Œ ì‚¬ìš©
                features = model.encode_image(images, proj_contrast=False, normalize=False)
            else:
                # UNI, Virchow2 (timm base)
                features = model(images)
            
            # ì €ì¥ (CPUë¡œ ì´ë™)
            features = features.cpu()
            
            for i, path in enumerate(paths):
                # ì›ë³¸ ê²½ë¡œ êµ¬ì¡° íŒŒì‹±
                rel_path = os.path.relpath(path, args.input_dir) # e.g., Case1/patch_01.png
                save_rel_path = os.path.splitext(rel_path)[0] + ".pt" # e.g., Case1/patch_01.pt
                save_path = os.path.join(args.output_dir, save_rel_path)
                
                # í•˜ìœ„ í´ë” ìƒì„±
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                
                # ì €ì¥
                torch.save(features[i].clone(), save_path)

    print("âœ… Extraction Complete!")

if __name__ == "__main__":
    main()