import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Sampler
from torchvision.utils import make_grid, save_image
import torch.nn.functional as F
from glob import glob
from tqdm import tqdm
import json
import random
import wandb 
from collections import defaultdict
import numpy as np
import wandb 

wandb.login(key="46aac2559a9feff8fff0ccf5e1c65a911aa3bd50")

# ==========================================================
# [ì„¤ì •]
# ==========================================================
TS_ROOT = "/mnt/d/workspace/dataset/TS_tokens"
BS_ROOT = "/mnt/d/workspace/dataset/BS_tokens"

TS_JSON = "./ts_train.json"
BS_JSON = "./bs_train.json"
TS_TEST_JSON = "./ts_test.json"

OUT_DIR = "./ckpt_feature_mapper_wsi"
DECODER_PATH = "checkpoints_token_decoder/decoder_model_epoch49.pth" 

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ = í•œ ìŠ¬ë¼ì´ë“œì—ì„œ ë³¼ íŒ¨ì¹˜ ê°œìˆ˜ (ë©”ëª¨ë¦¬ í—ˆìš© ë‚´ ìµœëŒ€ì¹˜ ì¶”ì²œ)
BATCH_SIZE = 256  
LR = 1e-4
EPOCHS = 200
DEVICE = "cuda"
SEED = 42

os.makedirs(OUT_DIR, exist_ok=True)

# ì‹œë“œ ê³ ì • (ìž¬í˜„ì„±)
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(SEED)

# ==========================================================
# 1. Slide-Aware Dataset & Sampler (í•µì‹¬ ê¸°ìˆ  ðŸ”¥)
# ==========================================================
class WSIDataset(Dataset):
    def __init__(self, root_dir, json_path, max_per_slide=1000):
        self.root_dir = root_dir
        with open(json_path, "r") as f:
            self.rel_paths = json.load(f)
            
        # í™˜ìž(Slide) IDë³„ë¡œ ì¸ë±ìŠ¤ ê·¸ë£¹í™”
        self.slide_indices = defaultdict(list)
        for idx, path in enumerate(self.rel_paths):
            # ê²½ë¡œ êµ¬ì¡°: SlideID/patch_x_y.pt
            slide_id = os.path.dirname(path)
            self.slide_indices[slide_id].append(idx)
            
        self.slide_ids = list(self.slide_indices.keys())
        print(f"âœ… Loaded {len(self.rel_paths)} patches from {len(self.slide_ids)} slides.")
        
        # Subsampling
        if max_per_slide is not None:
            new_rel_paths = []
            new_slide_indices = defaultdict(list)
            for slide_id, indices in self.slide_indices.items():
                if len(indices) > max_per_slide:
                    indices = random.sample(indices, max_per_slide)
                for idx in indices:
                    new_slide_indices[slide_id].append(len(new_rel_paths))
                    new_rel_paths.append(self.rel_paths[idx])
            self.rel_paths = new_rel_paths
            self.slide_indices = new_slide_indices
            print(f"ðŸ”ª Subsampled to {len(self.rel_paths)} patches after limiting {max_per_slide} per slide.")
            wandb.log({"subsampled_patches": len(self.rel_paths)})

    def __len__(self): return len(self.rel_paths)
    
    def __getitem__(self, idx):
        rel_path = self.rel_paths[idx]
        full_path = os.path.join(self.root_dir, rel_path)
        try:
            # [1536, 14, 14] -> [196, 1536]
            feat = torch.load(full_path, map_location="cpu")
            return feat.flatten(1).transpose(0, 1).float()
        except:
            return torch.zeros(196, 1536).float() # ì—ëŸ¬ ì²˜ë¦¬

class SlideBatchSampler(Sampler):
    """ 
    ëžœë¤í•˜ê²Œ ì„žì§€ ì•Šê³ , 'ê°™ì€ ìŠ¬ë¼ì´ë“œ'ì— ìžˆëŠ” íŒ¨ì¹˜ë“¤ì„ ë¬¶ì–´ì„œ ë°°ì¹˜ë¡œ ë‚´ë³´ëƒ„.
    ì´ê²Œ ìžˆì–´ì•¼ WSI-level Loss ê³„ì‚° ê°€ëŠ¥!
    """
    def __init__(self, slide_indices, batch_size):
        self.slide_indices = slide_indices
        self.batch_size = batch_size
        self.batches = []
        
        for slide_id, indices in slide_indices.items():
            # ìŠ¬ë¼ì´ë“œ ë‚´ì—ì„œëŠ” ìˆœì„œ ì„žê¸° (Patch Random)
            random.shuffle(indices)
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìžë¥´ê¸°
            for i in range(0, len(indices), batch_size):
                batch = indices[i:i+batch_size]
                # ë„ˆë¬´ ìž‘ì€ ìžíˆ¬ë¦¬ ë°°ì¹˜ëŠ” í•™ìŠµ ë¶ˆì•ˆì •í•˜ë¯€ë¡œ ìŠ¤í‚µ (ì„ íƒì‚¬í•­)
                if len(batch) > batch_size // 2: 
                    self.batches.append(batch)
        
        # ìŠ¬ë¼ì´ë“œ ìˆœì„œëŠ” ì„žìŒ (Slide Random)
        random.shuffle(self.batches)

    def __iter__(self):
        for batch in self.batches:
            yield batch

    def __len__(self):
        return len(self.batches)

# ==========================================================
# 2. Models
# ==========================================================
class FeatureMapper(nn.Module):
    """ TS -> BS ë³€í™˜ê¸° (Linear Projection + Residual) """
    def __init__(self, dim=1536):
        super().__init__()
        # ë‹¨ìˆœí• ìˆ˜ë¡ WSI ì„ í˜•ì„±ì„ ìž˜ ë³´ì¡´í•¨
        self.net = nn.Linear(dim, dim) 
    def forward(self, x):
        return x + self.net(x)

class Discriminator(nn.Module):
    """ Patch ë˜ëŠ” Slide ë²¡í„°ê°€ Realì¸ì§€ Fakeì¸ì§€ íŒë³„ """
    def __init__(self, dim=1536):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, 512), nn.LeakyReLU(0.2),
            nn.Linear(512, 256), nn.LeakyReLU(0.2),
            nn.Linear(256, 1)
        )
    def forward(self, x):
        return self.net(x)

# ë””ì½”ë” (ì‹œê°í™”ìš©)
class ResBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.block = nn.Sequential(nn.Conv2d(dim, dim, 3, 1, 1), nn.GroupNorm(32, dim), nn.ReLU(True), nn.Conv2d(dim, dim, 3, 1, 1), nn.GroupNorm(32, dim))
    def forward(self, x): return x + self.block(x)

class TokenDecoder(nn.Module):
    def __init__(self, in_channels=1536, out_channels=3):
        super().__init__()
        self.proj = nn.Conv2d(in_channels, 512, 1); self.first_norm = nn.GroupNorm(32, 512)
        self.up1 = nn.Sequential(nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.GroupNorm(32, 256), nn.ReLU(True), ResBlock(256))
        self.up2 = nn.Sequential(nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.GroupNorm(32, 128), nn.ReLU(True), ResBlock(128))
        self.up3 = nn.Sequential(nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.GroupNorm(32, 64), nn.ReLU(True), ResBlock(64))
        self.up4 = nn.Sequential(nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.GroupNorm(32, 32), nn.ReLU(True), ResBlock(32))
        self.final = nn.Sequential(nn.Conv2d(32, out_channels, 3, 1, 1), nn.Tanh())
    def forward(self, x):
        x = F.relu(self.first_norm(self.proj(x)))
        x = self.up1(x); x = self.up2(x); x = self.up3(x); x = self.up4(x)
        out = self.final(x)
        return F.interpolate(out, size=(256, 256), mode='bilinear')

# ==========================================================
# 3. Main Loop
# ==========================================================
def main():
    wandb.init(project="WSI-Level-Feature-Mapper")
    
    # 1. ë°ì´í„°ì…‹ & ìƒ˜í”ŒëŸ¬
    print("ðŸ“¦ Loading Datasets...")
    ds_ts = WSIDataset(TS_ROOT, TS_JSON, max_per_slide=500)
    ds_bs = WSIDataset(BS_ROOT, BS_JSON, max_per_slide=500)
    ds_test = WSIDataset(TS_ROOT, TS_TEST_JSON, max_per_slide=500) # Testìš©

    sampler_ts = SlideBatchSampler(ds_ts.slide_indices, BATCH_SIZE)
    sampler_bs = SlideBatchSampler(ds_bs.slide_indices, BATCH_SIZE) # Unpaired Sampling

    loader_ts = DataLoader(ds_ts, batch_sampler=sampler_ts, num_workers=4, pin_memory=True)
    loader_bs = DataLoader(ds_bs, batch_sampler=sampler_bs, num_workers=4, pin_memory=True)
    # TestëŠ” ê·¸ëƒ¥ ëžœë¤ ë¡œë” (ì‹œê°í™”ìš©)
    loader_test = DataLoader(ds_test, batch_size=4, shuffle=True, num_workers=2)

    bs_iter = iter(loader_bs)

    # 2. ëª¨ë¸
    mapper = FeatureMapper().to(DEVICE)
    patch_disc = Discriminator().to(DEVICE)
    wsi_disc = Discriminator().to(DEVICE) # [NEW] Slide Discriminator

    # ì‹œê°í™”ìš© ë””ì½”ë”
    decoder = None
    if os.path.exists(DECODER_PATH):
        decoder = TokenDecoder().to(DEVICE)
        decoder.load_state_dict(torch.load(DECODER_PATH, map_location=DEVICE))
        decoder.eval()
        print("âœ… Decoder Loaded.")

    opt_G = optim.Adam(mapper.parameters(), lr=LR, betas=(0.5, 0.999))
    opt_D = optim.Adam(list(patch_disc.parameters()) + list(wsi_disc.parameters()), lr=LR, betas=(0.5, 0.999))
    
    criterion_GAN = nn.MSELoss()

    print("ðŸ”¥ Start WSI-level Training...")
    
    for epoch in range(1, EPOCHS+1):
        mapper.train()
        loop = tqdm(loader_ts, desc=f"Ep {epoch}", ncols=100)
        
        for ts_batch in loop:
            # ts_batch: [B, 196, 1536] (í•œ ìŠ¬ë¼ì´ë“œì˜ íŒ¨ì¹˜ë“¤)
            ts_batch = ts_batch.to(DEVICE)
            
            # BS ë°°ì¹˜ (ë‹¤ë¥¸ ìŠ¬ë¼ì´ë“œì˜ íŒ¨ì¹˜ë“¤)
            try:
                bs_batch = next(bs_iter).to(DEVICE)
            except:
                bs_iter = iter(loader_bs)
                bs_batch = next(bs_iter).to(DEVICE)

            # --------------------
            # Train Discriminators
            # --------------------
            opt_D.zero_grad()
            
            fake_bs_batch = mapper(ts_batch)
            
            # [Patch Level]
            # ê° íŒ¨ì¹˜(196ê°œ í‰ê· )ê°€ BSìŠ¤ëŸ¬ìš´ê°€?
            ts_patch_vec = ts_batch.mean(dim=1) # [B, 1536]
            bs_patch_vec = bs_batch.mean(dim=1)
            fake_patch_vec = fake_bs_batch.detach().mean(dim=1)
            
            loss_D_patch = (criterion_GAN(patch_disc(bs_patch_vec), torch.ones_like(patch_disc(bs_patch_vec))) +
                            criterion_GAN(patch_disc(fake_patch_vec), torch.zeros_like(patch_disc(fake_patch_vec)))) / 2

            # [WSI Level] ðŸ”¥ í•µì‹¬
            # ë°°ì¹˜ ì „ì²´(Slide)ì˜ í‰ê·  ë²¡í„°ê°€ BSìŠ¤ëŸ¬ìš´ê°€?
            # [B, 196, 1536] -> [B, 1536] (Patch Mean) -> [1, 1536] (Slide Mean)
            real_wsi_vec = bs_patch_vec.mean(dim=0, keepdim=True) # Real BS Slide
            fake_wsi_vec = fake_patch_vec.mean(dim=0, keepdim=True) # Fake BS Slide
            
            loss_D_wsi = (criterion_GAN(wsi_disc(real_wsi_vec), torch.ones_like(wsi_disc(real_wsi_vec))) +
                          criterion_GAN(wsi_disc(fake_wsi_vec), torch.zeros_like(wsi_disc(fake_wsi_vec)))) / 2
            
            loss_D = loss_D_patch + loss_D_wsi
            loss_D.backward()
            opt_D.step()

            # --------------------
            # Train Mapper (G)
            # --------------------
            opt_G.zero_grad()
            
            # Regenerate fake (for grad)
            fake_bs_batch = mapper(ts_batch)
            fake_patch_vec = fake_bs_batch.mean(dim=1)
            fake_wsi_vec = fake_patch_vec.mean(dim=0, keepdim=True)
            
            # 1. GAN Loss (Patch & WSI)
            loss_G_patch = criterion_GAN(patch_disc(fake_patch_vec), torch.ones_like(patch_disc(fake_patch_vec)))
            loss_G_wsi = criterion_GAN(wsi_disc(fake_wsi_vec), torch.ones_like(wsi_disc(fake_wsi_vec)))
            
            # 2. Structural Consistency (Cosine with TS)
            # "Fake Slideì˜ ë°©í–¥ì„±ì€ ì›ë³¸ TS Slideì™€ ê°™ì•„ì•¼ í•œë‹¤" (êµ¬ì¡° ë³´ì¡´)
            ts_wsi_vec = ts_patch_vec.mean(dim=0, keepdim=True)
            loss_struct = 1 - F.cosine_similarity(fake_wsi_vec, ts_wsi_vec).mean()
            
            # 3. Distribution Matching
            loss_stats = F.mse_loss(fake_patch_vec.mean(0), bs_patch_vec.mean(0)) + \
                         F.mse_loss(fake_patch_vec.std(0), bs_patch_vec.std(0))

            loss_G = loss_G_patch + loss_G_wsi + (10.0 * loss_struct) + (10.0 * loss_stats)
            
            loss_G.backward()
            opt_G.step()
            
            # --------------------
            # Metric (ì¦ëª…ìš©)
            # --------------------
            # "Fakeê°€ TSë³´ë‹¤ BSì— ì–¼ë§ˆë‚˜ ê°€ê¹Œì›Œì¡Œë‚˜?" (L2 Distance)
            # (ìˆ˜ì¹˜ê°€ ìž‘ì•„ì§ˆìˆ˜ë¡ BSì— ê°€ê¹Œì›Œì§„ ê²ƒ)
            with torch.no_grad():
                dist_to_ts = F.mse_loss(fake_wsi_vec, ts_wsi_vec).item()
                dist_to_bs = F.mse_loss(fake_wsi_vec, real_wsi_vec).item()
                
            wandb.log({
                "Loss/G": loss_G.item(), "Loss/Struct": loss_struct.item(),
                "Dist/To_TS": dist_to_ts, "Dist/To_BS": dist_to_bs
            })
            loop.set_postfix(G=loss_G.item(), TS_dist=f"{dist_to_ts:.4f}", BS_dist=f"{dist_to_bs:.4f}")

        # ====================
        # Save & Visualize
        # ====================
        if epoch % 10 == 0:
            torch.save(mapper.state_dict(), os.path.join(OUT_DIR, f"mapper_ep{epoch}.pth"))
            
            if decoder is not None:
                mapper.eval()
                with torch.no_grad():
                    # Test Setì—ì„œ 4ìž¥ ê°€ì ¸ì˜¤ê¸°
                    ts_sample = next(iter(loader_test)).to(DEVICE) # [4, 196, 1536]
                    fake_sample = mapper(ts_sample)
                    
                    def to_img(feat):
                        B, N, C = feat.shape
                        H = int(N**0.5)
                        return decoder(feat.transpose(1, 2).reshape(B, C, H, H))

                    rec_ts = to_img(ts_sample)
                    fake_bs = to_img(fake_sample)
                    
                    grid = torch.cat([rec_ts, fake_bs], dim=2) # ìœ„ì•„ëž˜ ë§ê³  ì˜†ìœ¼ë¡œ ë¶™ìž„ (ë¹„êµìš©)
                    img_path = os.path.join(OUT_DIR, f"vis_ep{epoch}.png")
                    save_image(make_grid(grid, nrow=1, normalize=True), img_path)
                    
                    wandb.log({"Val": wandb.Image(img_path, caption=f"Ep{epoch}: TS(Left) -> FakeBS(Right)")})

if __name__ == "__main__":
    
    wandb.init(project="WSI-Level-Feature-Mapper")
    
    main()